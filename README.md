
# ğŸ“˜ Databricks Masterclass

A comprehensive guide based on the Databricks Masterclass, detailing the concepts and skills I acquired, including working with the Databricks File System (DBFS), accessing Azure Data Lake securely, utilizing Delta Lake for efficient data processing, and implementing streaming pipelines with Auto Loader.



---

## ğŸ“ Databricks File System (DBFS)
- DBFS is a distributed file system native to Databricks.
- Provides abstraction to treat storage like traditional filesystems (e.g., `/FileStore/raw.file.csv`).
- Connects securely to Delta Lake via URL-based paths.

---

## ğŸ” Hardcoded Access: Databricks to Azure Data Lake
- **Mounting** is used for persistent storage access.
- **Service Principal** grants programmatic access.
- **Steps to connect:**
  - Go to **Entra ID â†’ App Registration**
  - Assign **Storage Blob Data Contributor** role
  - Gather:
    - `Application_ID`
    - `Tenant_ID`
    - `Client Secret` or Certificate

---

## ğŸ› ï¸ Databricks Utilities
### ğŸ” File System Checks
```python
dbutils.fs.ls("abfss://source@storageaccount.dfs.core.windows.net/")
```

### ğŸ§® Parameterization with Widgets
```python
dbutils.widgets.text("p_name", "girald")
dbutils.widgets.get("p_name")
```

---

## ğŸ”‘ Secure Secrets with Azure Key Vault
### ğŸ—ï¸ Setup Process
1. **Azure Portal â†’ Key Vault â†’ Create**
2. Access Configuration â†’ Vault Access Policy
3. IAM â†’ Add Role: `Key Vault Administrator`
4. Create Secret (`app-secret`) with required value

### ğŸ”— Link Key Vault to Databricks
1. Navigate to Databricks URL â†’ `#secrets/createScope/`
2. Fill in:
   - **Scope Name**: `giraldScope`
   - **Manage Principal**: `Creator`
   - **DNS Name** and **Resource ID** from Key Vault â†’ Properties

### ğŸ” Accessing Secrets
```python
dbutils.secret.list(scope="giraldscope")
dbutils.secret.get(scope="giraldscope", key="app-secret")
```

---

## ğŸ“¥ Data Reading with PySpark
```python
df_sales = spark.read.format("csv") \
    .option("header", True) \
    .option("inferSchema", True) \
    .load("abfss://source@storagenameaccount.dfs.core.windows.net/")
```

---

## ğŸ”„ PySpark Transformations
```python
from pyspark.sql.functions import *
from pyspark.sql.types import *
```

### â— Split a Column
```python
df_sales.withColumn("testdata", split(col("testcolumn"), " ")).display()
```

### â• Add Constant
```python
df_sales.withColumn("testdata", lit("your_value"))
```

### ğŸ” Change Data Type
```python
df_sales.withColumn("testdata", col("testdata").cast(StringType())).display()
```

---

## ğŸ’¾ Delta Lake: Format & Transactions
- Delta Lake supports transactional logs: `.json`, `.crc`
- `%run` includes external notebook sessions
```python
%run "/foldernameofworkspace/filenameofnotebook"
```

### ğŸ“ Save as Delta
```python
df_sales.write.format("delta") \
    .mode("append") \
    .option("path", "abfss://foldername@storagename.dfs.core.windows.net/") \
    .save()
```

### ğŸ Use Catalog
```sql
USE CATALOG hive_metastore;
```

---

## ğŸ§  Delta Lake Advanced
### ğŸ“œ Versioning
```sql
DESCRIBE HISTORY table_name;
```

### ğŸ•’ Time Travel
```sql
RESTORE TABLE table_name TO VERSION AS OF 1;
```

### ğŸ§¹ Vacuum (Cleanup)
```sql
VACUUM table_name;
```
- Deletes old files (default: 7 days)

### âš™ï¸ Optimize
```sql
OPTIMIZE table_name;
```

### ğŸ”€ Z-Order Clustering
```sql
OPTIMIZE table_name ZORDER BY (id);
```

---

## ğŸŒŠ Auto Loader: Streaming Data
```python
df = spark.readStream.format("cloudFiles") \
    .option("cloudFiles.format", "parquet") \
    .option("cloudFiles.schemaLocation", "abfss://aldestination@datalake.dfs.core.windows.net/checkpoint") \
    .load("abfss://alsource@datalake.dfs.core.windows.net")

# Write stream

df.writeStream.format("delta") \
    .option("checkpointLocation", "abfss://aldestination@datalake.dfs.core.windows.net/checkpoint") \
    .option("mergeSchema", "true") \
    .trigger(processingTime="5 seconds") \
    .start("abfss://aldestination@datalake.dfs.core.windows.net/data")
```

---

## â›“ï¸ Workflows (Jobs)
- Automate notebook runs
- Schedule pipelines and manage dependencies
- Pass parameters between notebooks

---
Happy building! ğŸš€
